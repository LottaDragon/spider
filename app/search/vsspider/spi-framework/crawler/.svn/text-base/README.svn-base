========================================
crawler : asynchronous crawler 
author  : yue yifan(yueyifan@baidu.com)
========================================

----------
 function
----------
1. 异步连接/写http request/读http response
2. udp维护IP词典，lazy更新cache
3. 抓取线程数可配置，并发连接数可配置，页面抓取大小可配置，超时可配置
4. 支持3xx跳转和meta refresh的redirect抓取 

------
 note
------
1. 还没有用临时文件进行html page的存储，注意线程数、并发连接数、页面大小的配置
2. 如果抓取请求不是来自vsspider抓取环，请自觉控制抓取速度

-----------
 interface
-----------
 note:
 + mcapck2 protocal
 + "Pack_type" value refer to public/vs-spider.h

	1. cc -> crawler
	* Pack_type    (uint32) : PACK_CC2CR 
	* session_id   (uint32) 
	* url          (char *)  
	* pp_ip        (char *) : 下游地址  
	* pp_port      (int)    : 下游端口
	* is_cookie    (uint32) 
	* cookie       (char *) : if is_cookie equal 1, cookie is required 
	* is_referrer  (uint32)
	* referrer     (char *) : if is_referrer equal 1, referrer is required
	  s_ipaddr     (char *) : if no s_ipaddr, set s_ipaddr to default value 0
	  s_port       (int)    : if no s_port, set s_port to default value 0
	  method       (char *) : if no method, set method to "GET"
	  http_version (char *) : if no http_version, set to "HTTP/1.0"

	2. crawler -> pp 
	* request mcpack body : 请求的包体交给pp
	* Pack_type    (uint32) : PACK_CR2PP 
	* t_crw_in     (int64)  : 用来监控请求进入crawler时间
	* t_crw_out    (int64)  : 用来监控请求离开crawler时间
	* c_result     (int32)  : 抓取反馈,具体参考handler.h
	* c_statu      (int32)  : 2xx/3xx/...
	* c_page_len   (int32)  : 网页大小
	* c_last_mod   (char *) : 如果不为空就有,否则没有 last-modified, e.g. "Last Modified: Wed, 15 Apr 2009 12:16:41 GMT"
	* c_cont_type  (char *) : 如果不为空就有,否则没有 content type, e.g. "text/..." "image/..."
	* c_charset    (char *) : 如果不为空就有,否则没有 content type里面是否有charset字段
	* c_redir      (char *) : 如果不为空就有,否则没有 redirected url 重定向url
	* c_redir_trace(char *) : 如果不为空就有,否则没有 跳转过程,包括meta跳转和3xx跳转的url跳转trace
	* c_new_etag   (char *) : 如果不为空就有,否则没有 new etag
	* c_new_cookie (char *) : 如果不为空就有,否则没有 new cookie
	* c_cache_control (char *) : 如果不为空就有,否则没有 cache control
	* c_expires    (char *) : 如果不为空就有,否则没有 expires
	* c_page       (raw --> {void*,size} ) 网页内容 mcpack raw读写
	* c_header     (char *) : 如果不为空就有,否则没有 读取回来的http头部 

-------
 tools
-------
1. xclient
   演示怎么写一个crawler的上下游

2. filecralwer
   抓文件列表的crawler

3. nullcralwer
   伪造成crawler，啥事不干，可以用来测试cralwer数据接受功能

4. mocker_pp
   未造成pp，啥事不干，可以用来测试crawler的数据发送功能

5. shuffle_url
   利用hash特点，打散来自于同一个站点的url，便于打散站点抓取压力进行测试
